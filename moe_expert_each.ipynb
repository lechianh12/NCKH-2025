{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple baseline with Landsat and Bioclimatic Cubes + Sentinel images [0.31626]\n",
    "\n",
    "Following the three provided baselies with different modalities, we have provide a multimodal approch based on \"siamiese\" network with multiple inputs and simple shared \"decoder\". The links for the separated baselines are as follows:\n",
    "\n",
    "- [Baseline with Bioclimatic Cubes [0.25784]](https://www.kaggle.com/code/picekl/baseline-with-bioclimatic-cubes-0-25784)\n",
    "- [Baseline with Landsat Cubes [0.26424]](https://www.kaggle.com/code/picekl/baseline-with-landsat-cubes-0-26424)\n",
    "- [Baseline with Sentinel Images [0.23594]](https://www.kaggle.com/code/picekl/baseline-with-sentinel-images-0-23594)\n",
    "\n",
    "**Considering the significant extent for enhancing performance of this baseline, we encourage you to experiment with various techniques, architectures, losses, etc.**\n",
    "\n",
    "#### **Have Fun!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T13:30:07.054038Z",
     "iopub.status.busy": "2024-05-01T13:30:07.053659Z",
     "iopub.status.idle": "2024-05-01T13:30:07.058148Z",
     "shell.execute_reply": "2024-05-01T13:30:07.057269Z",
     "shell.execute_reply.started": "2024-05-01T13:30:07.054008Z"
    }
   },
   "source": [
    "# Data description\n",
    "\n",
    "## Landsat time series\n",
    "\n",
    "Satellite time series data includes over 20 years of Landsat satellite imagery extracted from [Ecodatacube](https://stac.ecodatacube.eu/).\n",
    "The data was acquired through the Landsat satellite program and pre-processed by Ecodatacube to produce raster files scaled to the entire European continent and projected into a unique CRS.\n",
    "\n",
    "Since the original rasters require a high amount of disk space, we extracted the data points from each spectral band corresponding to all PA and PO locations (i.e., GPS coordinates) and aggregated them in (i) CSV files and (ii) data cubes as tensor objects. Each data point corresponds to the mean value of Landsat's observations at the given location for three months before the given time; e.g., the value of a time series element under column 2012_4 will represent the mean value for that element from October 2012 to December 2012.\n",
    "\n",
    "In this notebook, we will work with just the cubes. The cubes are structured as follows.\n",
    "**Shape**: `(n_bands, n_quarters, n_years)` where:\n",
    "- `n_bands` = 6 comprising [`red`, `green`, `blue`, `nir`, `swir1`, `swir2`]\n",
    "- `n_quarters` = 4 \n",
    "    - *Quarter 1*: December 2 of previous year until March 20 of current year (winter season proxy),\n",
    "    - *Quarter 2*: March 21 until June 24 of current year (spring season proxy),\n",
    "    - *Quarter 3*: June 25 until September 12 of current year (summer season proxy),\n",
    "    - *Quarter 4*: September 13 until December 1 of current year (fall season proxy).\n",
    "- `n_years` = 21 (ranging from 2000 to 2020)\n",
    "\n",
    "The datacubes can simply be loaded as tensors using PyTorch with the following command :\n",
    "\n",
    "```python\n",
    "import torch\n",
    "torch.load('path_to_file.pt')\n",
    "```\n",
    "\n",
    "**References:**\n",
    "- *Traceability (lineage): This dataset is a seasonally aggregated and gapfilled version of the Landsat GLAD analysis-ready data product presented by Potapov et al., 2020 ( https://doi.org/10.3390/rs12030426 ).*\n",
    "- *Scientific methodology: The Landsat GLAD ARD dataset was aggregated and harmonized using the eumap python package (available at https://eumap.readthedocs.io/en/latest/ ). The full process of gapfilling and harmonization is described in detail in Witjes et al., 2022 (in review, preprint available at https://doi.org/10.21203/rs.3.rs-561383/v3 ).*\n",
    "- *Ecodatacube.eu: Analysis-ready open environmental data cube for Europe (https://doi.org/10.21203/rs.3.rs-2277090/v3).*\n",
    "\n",
    "\n",
    "## Bioclimatic time series\n",
    "\n",
    "The Bioclimatic Cubes are created from **four** monthly GeoTIFF CHELSA (https://chelsa-climate.org/timeseries/) time series climatic rasters with a resolution of 30 arc seconds, i.e. approximately 1km. The four variables are the precipitation (pr), maximum- (taxmax), minimum- (tasmin), and mean (tax) daily temperatures per month from January 2000 to June 2019. We provide the data in three forms: (i) raw rasters (GeoTiff images), (ii) CSV file with pre-extracted values for each location, i.e., surveyId, and (iii) data cubes as tensor object (.pt).\n",
    "\n",
    "In this notebook, we will work with just the cubes. The cubes are structured as follows.\n",
    "**Shape**: `(n_year, n_month, n_bio)` where:\n",
    "- `n_year` = 19 (ranging from 2000 to 2018)\n",
    "- `n_month` = 12 (ranging from January 01 to December 12)\n",
    "- `n_bio` = 4 comprising [`pr` (precipitation), `tas` (mean daily air temperature), `tasmin`, `tasmax`]\n",
    "\n",
    "The datacubes can simply be loaded as tensors using PyTorch with the following command :\n",
    "\n",
    "```python\n",
    "import torch\n",
    "torch.load('path_to_file.pt')\n",
    "```\n",
    "\n",
    "**References:**\n",
    "- *Karger, D.N., Conrad, O., Böhner, J., Kawohl, T., Kreft, H., Soria-Auza, R.W., Zimmermann, N.E., Linder, P., Kessler, M. (2017): Climatologies at high resolution for the Earth land surface areas. Scientific Data. 4 170122. https://doi.org/10.1038/sdata.2017.122*\n",
    "\n",
    "- *Karger D.N., Conrad, O., Böhner, J., Kawohl, T., Kreft, H., Soria-Auza, R.W., Zimmermann, N.E, Linder, H.P., Kessler, M. Data from: Climatologies at high resolution for the earth’s land surface areas. Dryad Digital Repository. http://dx.doi.org/doi:10.5061/dryad.kd1d4*\n",
    "\n",
    "\n",
    "## Sentinel Image Patches\n",
    "\n",
    "The Sentinel Image data was acquired through the Sentinel2 satellite program and pre-processed by [Ecodatacube](https://stac.ecodatacube.eu/) to produce raster files scaled to the entire European continent and projected into a unique CRS. We filtered the data in order to pick patches from each spectral band corresponding to a location ((lon, lat) GPS coordinates) and a date matching that of our occurrences', and split them into JPEG files (RGB in 3-channels .jpeg files and NIR in single-channel .jpeg files) with a 128x128 resolution. The images were converted from sentinel uint15 to uint8 by clipping data pixel values over 10000 and applying a gamma correction of 2.5.\n",
    "\n",
    "The data can simply be loaded using the following method:\n",
    "\n",
    "```python\n",
    "def construct_patch_path(output_path, survey_id):\n",
    "    \"\"\"Construct the patch file path based on survey_id as './CD/AB/XXXXABCD.jpeg'\"\"\"\n",
    "    path = output_path\n",
    "    for d in (str(survey_id)[-2:], str(survey_id)[-4:-2]):\n",
    "        path = os.path.join(path, d)\n",
    "\n",
    "    path = os.path.join(path, f\"{survey_id}.jpeg\")\n",
    "\n",
    "    return path\n",
    "```\n",
    "\n",
    "**References:**\n",
    "- *Traceability (lineage): The dataset was produced entirely by mosaicking and seasonally aggregating imagery from the Sentinel-2 Level-2A product (https://sentinels.copernicus.eu/web/sentinel/user-guides/sentinel-2-msi/product-types/level-2a)*\n",
    "- *Ecodatacube.eu: Analysis-ready open environmental data cube for Europe (https://doi.org/10.21203/rs.3.rs-2277090/v3)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:07.29831Z",
     "start_time": "2024-04-30T21:25:05.354584Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-21T03:17:12.723959Z",
     "iopub.status.busy": "2025-04-21T03:17:12.723345Z",
     "iopub.status.idle": "2025-04-21T03:17:18.443298Z",
     "shell.execute_reply": "2025-04-21T03:17:18.442173Z",
     "shell.execute_reply.started": "2025-04-21T03:17:12.723883Z"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import warnings\n",
    "import timm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare custom dataset loader\n",
    "\n",
    "We have to slightly update the Dataset to provide the relevant data in the appropriate format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:32.627928Z",
     "start_time": "2024-04-30T21:25:32.612131Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-21T03:17:18.445725Z",
     "iopub.status.busy": "2025-04-21T03:17:18.445233Z",
     "iopub.status.idle": "2025-04-21T03:17:18.466555Z",
     "shell.execute_reply": "2025-04-21T03:17:18.465322Z",
     "shell.execute_reply.started": "2025-04-21T03:17:18.445696Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def construct_patch_path(data_path, survey_id):\n",
    "    \"\"\"Construct the patch file path based on plot_id as './CD/AB/XXXXABCD.jpeg'\"\"\"\n",
    "    path = data_path\n",
    "    for d in (str(survey_id)[-2:], str(survey_id)[-4:-2]):\n",
    "        path = os.path.join(path, d)\n",
    "\n",
    "    path = os.path.join(path, f\"{survey_id}.jpeg\")\n",
    "\n",
    "    return path\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, bioclim_data_dir, landsat_data_dir, sentinel_data_dir, metadata, transform=None):\n",
    "        self.transform = transform\n",
    "        self.sentinel_transform = transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.5, 0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "      \n",
    "        self.bioclim_data_dir = bioclim_data_dir\n",
    "        self.landsat_data_dir = landsat_data_dir\n",
    "        self.sentinel_data_dir = sentinel_data_dir\n",
    "        self.metadata = metadata\n",
    "        self.metadata = self.metadata.dropna(subset=\"speciesId\").reset_index(drop=True)\n",
    "        self.metadata['speciesId'] = self.metadata['speciesId'].astype(int)\n",
    "        self.label_dict = self.metadata.groupby('surveyId')['speciesId'].apply(list).to_dict()\n",
    "        \n",
    "        self.metadata = self.metadata.drop_duplicates(subset=\"surveyId\").reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "        \n",
    "        landsat_sample = torch.nan_to_num(torch.load(os.path.join(self.landsat_data_dir, f\"GLC24-PA-train-landsat-time-series_{survey_id}_cube.pt\")))\n",
    "        bioclim_sample = torch.nan_to_num(torch.load(os.path.join(self.bioclim_data_dir, f\"GLC24-PA-train-bioclimatic_monthly_{survey_id}_cube.pt\")))\n",
    "\n",
    "        rgb_sample = np.array(Image.open(construct_patch_path(self.sentinel_data_dir, survey_id)))\n",
    "        nir_sample = np.array(Image.open(construct_patch_path(self.sentinel_data_dir.replace(\"rgb\", \"nir\").replace(\"RGB\", \"NIR\"), survey_id)))\n",
    "        sentinel_sample = np.concatenate((rgb_sample, nir_sample[...,None]), axis=2)\n",
    "\n",
    "        species_ids = self.label_dict.get(survey_id, [])  # Get list of species IDs for the survey ID\n",
    "        label = torch.zeros(num_classes)  # Initialize label tensor\n",
    "        for species_id in species_ids:\n",
    "            label_id = species_id\n",
    "            label[label_id] = 1  # Set the corresponding class index to 1 for each species\n",
    "        \n",
    "        if isinstance(landsat_sample, torch.Tensor):\n",
    "            landsat_sample = landsat_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            landsat_sample = landsat_sample.numpy()  # Convert tensor to numpy array\n",
    "            \n",
    "        if isinstance(bioclim_sample, torch.Tensor):\n",
    "            bioclim_sample = bioclim_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            bioclim_sample = bioclim_sample.numpy()  # Convert tensor to numpy array   \n",
    "        \n",
    "        if self.transform:\n",
    "            landsat_sample = self.transform(landsat_sample)\n",
    "            bioclim_sample = self.transform(bioclim_sample)\n",
    "            sentinel_sample = self.sentinel_transform(sentinel_sample)\n",
    "\n",
    "        return landsat_sample, bioclim_sample, sentinel_sample, label, survey_id\n",
    "\n",
    "class TestDataset(TrainDataset):\n",
    "    def __init__(self, bioclim_data_dir, landsat_data_dir, sentinel_data_dir, metadata, transform=None):\n",
    "        self.transform = transform\n",
    "        self.sentinel_transform = transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.5, 0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "      \n",
    "        self.bioclim_data_dir = bioclim_data_dir\n",
    "        self.landsat_data_dir = landsat_data_dir\n",
    "        self.sentinel_data_dir = sentinel_data_dir\n",
    "        self.metadata = metadata\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "        landsat_sample = torch.nan_to_num(torch.load(os.path.join(self.landsat_data_dir, f\"GLC24-PA-test-landsat_time_series_{survey_id}_cube.pt\")))\n",
    "        bioclim_sample = torch.nan_to_num(torch.load(os.path.join(self.bioclim_data_dir, f\"GLC24-PA-test-bioclimatic_monthly_{survey_id}_cube.pt\")))\n",
    "        \n",
    "        rgb_sample = np.array(Image.open(construct_patch_path(self.sentinel_data_dir, survey_id)))\n",
    "        nir_sample = np.array(Image.open(construct_patch_path(self.sentinel_data_dir.replace(\"rgb\", \"nir\").replace(\"RGB\", \"NIR\"), survey_id)))\n",
    "        sentinel_sample = np.concatenate((rgb_sample, nir_sample[...,None]), axis=2)\n",
    "\n",
    "        if isinstance(landsat_sample, torch.Tensor):\n",
    "            landsat_sample = landsat_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            landsat_sample = landsat_sample.numpy()  # Convert tensor to numpy array\n",
    "            \n",
    "        if isinstance(bioclim_sample, torch.Tensor):\n",
    "            bioclim_sample = bioclim_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            bioclim_sample = bioclim_sample.numpy()  # Convert tensor to numpy array   \n",
    "        \n",
    "        if self.transform:\n",
    "            landsat_sample = self.transform(landsat_sample)\n",
    "            bioclim_sample = self.transform(bioclim_sample)\n",
    "            sentinel_sample = self.sentinel_transform(sentinel_sample)\n",
    "\n",
    "        return landsat_sample, bioclim_sample, sentinel_sample, survey_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load metadata and prepare data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:34.532017Z",
     "start_time": "2024-04-30T21:25:32.615562Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-21T03:17:18.467998Z",
     "iopub.status.busy": "2025-04-21T03:17:18.467672Z",
     "iopub.status.idle": "2025-04-21T03:17:23.228597Z",
     "shell.execute_reply": "2025-04-21T03:17:23.227218Z",
     "shell.execute_reply.started": "2025-04-21T03:17:18.467972Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dataset and DataLoader\n",
    "batch_size = 128\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load Training metadata\n",
    "train_landsat_data_path = \"/home/le-chi-anh/Downloads/geolifeclef-2024/TimeSeries-Cubes/TimeSeries-Cubes/GLC24-PA-train-landsat_time_series/\"\n",
    "train_bioclim_data_path = \"/home/le-chi-anh/Downloads/geolifeclef-2024/TimeSeries-Cubes/TimeSeries-Cubes/GLC24-PA-train-bioclimatic_monthly/\"\n",
    "train_sentinel_data_path=\"/home/le-chi-anh/Downloads/geolifeclef-2024/PA_Train_SatellitePatches_RGB/pa_train_patches_rgb/\"\n",
    "train_metadata_path = \"/home/le-chi-anh/Downloads/geolifeclef-2024/GLC24_PA_metadata_train.csv\"\n",
    "\n",
    "train_metadata = pd.read_csv(train_metadata_path)\n",
    "dataset_alpine = TrainDataset(train_bioclim_data_path, train_landsat_data_path, train_sentinel_data_path, train_metadata, transform=transform)\n",
    "train_loader = DataLoader(dataset_alpine, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# Load Test metadata\n",
    "test_landsat_data_path = \"/home/le-chi-anh/Downloads/geolifeclef-2024/TimeSeries-Cubes/TimeSeries-Cubes/GLC24-PA-test-landsat_time_series/\"\n",
    "test_bioclim_data_path = \"/home/le-chi-anh/Downloads/geolifeclef-2024/TimeSeries-Cubes/TimeSeries-Cubes/GLC24-PA-test-bioclimatic_monthly/\"\n",
    "test_sentinel_data_path = \"/home/le-chi-anh/Downloads/geolifeclef-2024/PA_Test_SatellitePatches_RGB/pa_test_patches_rgb/\"\n",
    "test_metadata_path = \"/home/le-chi-anh/Downloads/geolifeclef-2024/GLC24_PA_metadata_test.csv\"\n",
    "\n",
    "test_metadata = pd.read_csv(test_metadata_path)\n",
    "test_dataset = TestDataset(test_bioclim_data_path, test_landsat_data_path, test_sentinel_data_path, test_metadata, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and initialize a Multimodal Model\n",
    "\n",
    "To process multiple inputs with different modalities and formats we use so-call siamiese approach where each modality is processed with different backbone (i.e., encoder). Data encoded into a 1d vector are concatenated and classified with a simple fully connected neural network. Short recap from previous notebooks.\n",
    "- The Landsat cubes have a shape of [6,4,21] (BANDs, QUARTERs, and YEARs).\n",
    "- The Bioclimatic cubes have a shape of [4,19,12] (RASTER-TYPE, YEAR, and MONTH)\n",
    "- The Sentinel Image Patches have a shape od [128, 128, 4] (R, G, B, NIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:31.014067Z",
     "start_time": "2024-04-30T21:25:31.01006Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-21T03:19:34.477873Z",
     "iopub.status.busy": "2025-04-21T03:19:34.477477Z",
     "iopub.status.idle": "2025-04-21T03:19:34.489054Z",
     "shell.execute_reply": "2025-04-21T03:19:34.487906Z",
     "shell.execute_reply.started": "2025-04-21T03:19:34.477814Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torchvision.models as models\n",
    "# from torchvision.models import swin_t\n",
    "\n",
    "# # --- Fix the Expert and MoE classes first (typo) ---\n",
    "\n",
    "# class SimpleMLPExpert(nn.Module):\n",
    "#     # FIX: Change init to __init__\n",
    "#     def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "#         super(SimpleMLPExpert, self).__init__()\n",
    "#         self.network = nn.Sequential(\n",
    "#             nn.Linear(input_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, output_dim)\n",
    "#         )\n",
    "#     def forward(self, x):\n",
    "#         return self.network(x)\n",
    "\n",
    "# class MoELayer(nn.Module):\n",
    "#     # FIX: Change init to __init__\n",
    "#     def __init__(self, input_dim, output_dim, num_experts, hidden_dim_mlp_expert):\n",
    "#         super(MoELayer, self).__init__()\n",
    "#         self.num_experts = num_experts\n",
    "#         self.gate = nn.Linear(input_dim, num_experts)\n",
    "#         self.experts = nn.ModuleList([\n",
    "#             SimpleMLPExpert(input_dim, output_dim, hidden_dim_mlp_expert)\n",
    "#             for _ in range(num_experts)\n",
    "#         ])\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         batch_size = x.size(0)\n",
    "#         # x should have shape [batch_size, input_dim]\n",
    "#         gate_scores = F.softmax(self.gate(x), dim=-1) # [batch_size, num_experts]\n",
    "#         expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=1) # [batch_size, num_experts, output_dim]\n",
    "#         gate_scores_unsqueezed = gate_scores.unsqueeze(-1) # [batch_size, num_experts, 1]\n",
    "#         output = torch.sum(gate_scores_unsqueezed * expert_outputs, dim=1) # [batch_size, output_dim]\n",
    "#         return output\n",
    "\n",
    "# # --- Fix the Feature Extractor Experts (typo + Swin head) ---\n",
    "\n",
    "# class LandsatExpert(nn.Module):\n",
    "#     # FIX: Change init to __init__\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.model = models.resnet18(weights=None)\n",
    "#         self.model.conv1 = nn.Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "#         # Correctly removed head for features\n",
    "#         self.model.fc = nn.Identity() # Output dimension: 512\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "# class BioclimExpert(nn.Module):\n",
    "#     # FIX: Change init to __init__\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.model = models.resnet18(weights=None)\n",
    "#         self.model.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "#         # Correctly removed head for features\n",
    "#         self.model.fc = nn.Identity() # Output dimension: 512\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "# class SentinelExpert(nn.Module):\n",
    "#     # FIX: Change init to __init__\n",
    "#     def __init__(self):\n",
    "#         # FIX: Change init to __init__\n",
    "#         super(SentinelExpert, self).__init__()\n",
    "\n",
    "#         self.model = swin_t(weights=None)\n",
    "\n",
    "#         # Sửa Conv2d đầu tiên để nhận 4 channels\n",
    "#         self.model.features[0][0] = nn.Conv2d(\n",
    "#             in_channels=4,\n",
    "#             out_channels=96,\n",
    "#             kernel_size=4,\n",
    "#             stride=4\n",
    "#         )\n",
    "\n",
    "#         # --- FIX: Replace the classification head with Identity ---\n",
    "#         # This ensures the model returns the feature vector (768 dim)\n",
    "#         # instead of the classification scores (1000 dim by default).\n",
    "#         self.model.head = nn.Identity()\n",
    "\n",
    "#         # Output dimension of Swin tiny features is 768\n",
    "#         self.output_dim = 768\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x: (batch_size, 4, H, W) - e.g., (batch_size, 4, 128, 128)\n",
    "#         # After passing through self.model (with head=Identity),\n",
    "#         # the output should be the pooled features: (batch_size, 768)\n",
    "#         x = self.model(x)\n",
    "\n",
    "#         # The if x.ndim == 3: block is likely unnecessary if head is Identity\n",
    "#         # as the model's internal forward pass typically handles pooling before the head.\n",
    "#         # If you are certain your Swin version/config returns 3D [B, L, E] here,\n",
    "#         # then the mean operation is correct, but let's assume Identity gives [B, E].\n",
    "#         # I'll remove it for standard Swin head replacement.\n",
    "#         # if x.ndim == 3:\n",
    "#         #     x = x.mean(dim=1) # Pool seq_len dimension\n",
    "\n",
    "#         # x should now be (batch_size, 768)\n",
    "#         return x\n",
    "\n",
    "# # --- Correct the Multimodal Ensemble (typo) ---\n",
    "\n",
    "# class MultimodalEnsemble(nn.Module):\n",
    "#     # FIX: Change init to __init__\n",
    "#     def __init__(self, num_classes, num_experts_per_modality=4, hidden_dim_mlp_expert=512, moe_output_dim=1024):\n",
    "#         # FIX: Change init to __init__\n",
    "#         super(MultimodalEnsemble, self).__init__()\n",
    "\n",
    "#         # Initialize feature extractors (using the corrected classes)\n",
    "#         self.landsat_expert = LandsatExpert()\n",
    "#         self.bioclim_expert = BioclimExpert()\n",
    "#         self.sentinel_expert = SentinelExpert() # This will now return 768 features\n",
    "\n",
    "#         # Feature dims\n",
    "#         landsat_dim = 512\n",
    "#         bioclim_dim = 512\n",
    "#         sentinel_dim = 768 # Correct dimension for Swin features\n",
    "\n",
    "#         # Initialize MoE layers for each modality\n",
    "#         # The input_dim for these MoE layers must match the output of the Experts\n",
    "#         self.landsat_moe = MoELayer(landsat_dim, moe_output_dim, num_experts_per_modality, hidden_dim_mlp_expert)\n",
    "#         self.bioclim_moe = MoELayer(bioclim_dim, moe_output_dim, num_experts_per_modality, hidden_dim_mlp_expert)\n",
    "#         self.sentinel_moe = MoELayer(sentinel_dim, moe_output_dim, num_experts_per_modality, hidden_dim_mlp_expert) # Input dim is now correctly 768\n",
    "\n",
    "#         # Final classifier\n",
    "#         fusion_input_dim = moe_output_dim * 3\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.LayerNorm(fusion_input_dim),\n",
    "#             nn.Linear(fusion_input_dim, fusion_input_dim // 2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.Linear(fusion_input_dim // 2, num_classes)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x_landsat, x_bioclim, x_sentinel):\n",
    "#         # Feature extraction\n",
    "#         landsat_feat = self.landsat_expert(x_landsat) # -> [batch, 512]\n",
    "#         bioclim_feat = self.bioclim_expert(x_bioclim) # -> [batch, 512]\n",
    "#         sentinel_feat = self.sentinel_expert(x_sentinel) # -> [batch, 768]\n",
    "\n",
    "#         # MoE processing\n",
    "#         landsat_moe_out = self.landsat_moe(landsat_feat) # -> [batch, moe_output_dim]\n",
    "#         bioclim_moe_out = self.bioclim_moe(bioclim_feat) # -> [batch, moe_output_dim]\n",
    "#         sentinel_moe_out = self.sentinel_moe(sentinel_feat) # -> [batch, moe_output_dim]\n",
    "\n",
    "#         # Fusion\n",
    "#         fusion = torch.cat([landsat_moe_out, bioclim_moe_out, sentinel_moe_out], dim=1) # -> [batch, moe_output_dim * 3]\n",
    "\n",
    "#         # Classification\n",
    "#         out = self.classifier(fusion) # -> [batch, num_classes]\n",
    "\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.models import swin_t\n",
    "\n",
    "# --- Helper Expert Class (unchanged) ---\n",
    "\n",
    "class SimpleMLPExpert(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        super(SimpleMLPExpert, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# --- Top-K Mixture of Experts Layer (unchanged) ---\n",
    "\n",
    "class TopKMoELayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_experts, hidden_dim_mlp_expert, k=2):\n",
    "        \"\"\"\n",
    "        Top-K Mixture of Experts Layer.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Dimension of the input feature vector.\n",
    "            output_dim (int): Dimension of the output feature vector.\n",
    "            num_experts (int): Total number of experts.\n",
    "            hidden_dim_mlp_expert (int): Hidden dimension for the MLP experts.\n",
    "            k (int): Number of top experts to select for each input sample. Defaults to 2.\n",
    "        \"\"\"\n",
    "        super(TopKMoELayer, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.output_dim = output_dim\n",
    "        self.k = max(1, min(k, num_experts)) # Ensure k is valid\n",
    "\n",
    "        self.gate = nn.Linear(input_dim, num_experts)\n",
    "        self.experts = nn.ModuleList([\n",
    "            SimpleMLPExpert(input_dim, output_dim, hidden_dim_mlp_expert)\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        if x.ndim != 2 or x.size(1) != self.gate.in_features:\n",
    "             raise ValueError(f\"Input shape must be [batch_size, {self.gate.in_features}], but got {x.shape}\")\n",
    "\n",
    "        gate_scores = self.gate(x) # [batch_size, num_experts]\n",
    "        topk_scores, topk_indices = torch.topk(gate_scores, self.k, dim=-1) # [batch_size, k], [batch_size, k]\n",
    "        topk_gate_weights = F.softmax(topk_scores, dim=-1) # [batch_size, k]\n",
    "\n",
    "        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=1) # [batch_size, num_experts, output_dim]\n",
    "\n",
    "        topk_indices_expanded = topk_indices.unsqueeze(-1).expand(-1, -1, self.output_dim) # [batch_size, k, output_dim]\n",
    "        selected_expert_outputs = torch.gather(expert_outputs, dim=1, index=topk_indices_expanded) # [batch_size, k, output_dim]\n",
    "\n",
    "        topk_gate_weights_unsqueezed = topk_gate_weights.unsqueeze(-1) # [batch_size, k, 1]\n",
    "        weighted_outputs = topk_gate_weights_unsqueezed * selected_expert_outputs # [batch_size, k, output_dim]\n",
    "\n",
    "        output = torch.sum(weighted_outputs, dim=1) # [batch_size, output_dim]\n",
    "\n",
    "        return output\n",
    "\n",
    "# --- Feature Extractor Experts (Corrected SentinelExpert) ---\n",
    "\n",
    "class LandsatExpert(nn.Module):\n",
    "    \"\"\"ResNet18 based expert for Landsat data (6 channels).\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = models.resnet18(weights=None)\n",
    "        self.model.conv1 = nn.Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.model.fc = nn.Identity() # Output dimension: 512\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class BioclimExpert(nn.Module):\n",
    "    \"\"\"ResNet18 based expert for Bioclim data (4 channels).\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = models.resnet18(weights=None)\n",
    "        self.model.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.model.fc = nn.Identity() # Output dimension: 512\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class SentinelExpert(nn.Module):\n",
    "    \"\"\"Swin Transformer based expert for Sentinel data (4 channels).\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = swin_t(weights=None)\n",
    "\n",
    "        # Modify the first convolutional layer (PatchEmbed) to accept 4 input channels\n",
    "        # The Swin Transformer starts with a PatchEmbed layer (often a Sequential module).\n",
    "        # The Conv2d is usually the first element in this Sequential.\n",
    "        patch_embed_layer = self.model.features[0]\n",
    "        if isinstance(patch_embed_layer, nn.Sequential) and isinstance(patch_embed_layer[0], nn.Conv2d):\n",
    "             original_conv = patch_embed_layer[0]\n",
    "             # --- FIX: Pass a boolean (original_conv.bias is not None) to the bias argument ---\n",
    "             self.model.features[0][0] = nn.Conv2d(\n",
    "                in_channels=4,\n",
    "                out_channels=original_conv.out_channels,\n",
    "                kernel_size=original_conv.kernel_size,\n",
    "                stride=original_conv.stride,\n",
    "                padding=original_conv.padding,\n",
    "                bias=original_conv.bias is not None # THIS IS THE FIX\n",
    "            )\n",
    "        else:\n",
    "            # Raise an error if the structure is not as expected for swin_t\n",
    "            raise TypeError(\"Expected the first layer of Swin features to be a Sequential containing Conv2d\")\n",
    "\n",
    "        # Replace the classification head with Identity.\n",
    "        self.model.head = nn.Identity()\n",
    "\n",
    "        # The output dimension of Swin-Tiny features after pooling is 768\n",
    "        self.output_dim = 768\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torchvision.models as models\n",
    "# from torchvision.models import resnet18, ResNet18_Weights\n",
    "# import warnings\n",
    "\n",
    "# class SentinelExpert(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # Target input channels\n",
    "#         target_in_chans = 4\n",
    "\n",
    "#         try:\n",
    "#             # Attempt to load Sentinel2_RGB_MOCO weights\n",
    "#             if hasattr(ResNet18_Weights, 'SENTINEL2_RGB_MOCO'):\n",
    "#                 weights_enum = ResNet18_Weights.SENTINEL2_RGB_MOCO\n",
    "#                 weights = weights_enum.DEFAULT\n",
    "#                 print(\"Loaded ResNet18_Weights.SENTINEL2_RGB_MOCO for pretraining.\")\n",
    "#             else:\n",
    "#                 warnings.warn(\"SENTINEL2_RGB_MOCO not found, falling back to random init.\")\n",
    "#                 weights = None\n",
    "#         except Exception as e:\n",
    "#             warnings.warn(f\"Failed to load weights due to {e}. Falling back to random init.\")\n",
    "#             weights = None\n",
    "\n",
    "#         # Load the ResNet18 model\n",
    "#         self.model = resnet18(weights=weights)\n",
    "\n",
    "#         # Adapt conv1 to accept 4 channels if needed\n",
    "#         if self.model.conv1.in_channels != target_in_chans:\n",
    "#             old_conv = self.model.conv1\n",
    "#             new_conv = nn.Conv2d(\n",
    "#                 in_channels=target_in_chans,\n",
    "#                 out_channels=old_conv.out_channels,\n",
    "#                 kernel_size=old_conv.kernel_size,\n",
    "#                 stride=old_conv.stride,\n",
    "#                 padding=old_conv.padding,\n",
    "#                 bias=(old_conv.bias is not None)\n",
    "#             )\n",
    "\n",
    "#             # Initialize new_conv weights\n",
    "#             with torch.no_grad():\n",
    "#                 # Copy pretrained weights for first 3 channels\n",
    "#                 new_conv.weight[:, :3, :, :] = old_conv.weight\n",
    "#                 # Initialize the 4th channel as zeros\n",
    "#                 new_conv.weight[:, 3:, :, :] = 0.0\n",
    "#                 if old_conv.bias is not None:\n",
    "#                     new_conv.bias = old_conv.bias\n",
    "\n",
    "#             self.model.conv1 = new_conv\n",
    "#             print(f\"Adapted conv1 to {target_in_chans} input channels.\")\n",
    "\n",
    "#         else:\n",
    "#             print(f\"Model conv1 already has {target_in_chans} input channels.\")\n",
    "\n",
    "#         # Remove the classifier head (fc layer)\n",
    "#         self.model.fc = nn.Identity()\n",
    "#         print(\"Removed final classifier layer (fc).\")\n",
    "\n",
    "#         # # Output dimension after global average pooling\n",
    "#         # self.sentinel_feat_dim = 512\n",
    "#         # print(f\"Feature dimension: {self.sentinel_feat_dim}\")\n",
    "\n",
    "#         self.output_dim = 512\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         features = self.model(x)\n",
    "#         return features\n",
    "\n",
    "\n",
    "\n",
    "# --- Multimodal Ensemble Model (unchanged logic, uses corrected SentinelExpert) ---\n",
    "\n",
    "class MultimodalEnsemble(nn.Module):\n",
    "    def __init__(self, num_classes, num_experts_per_modality=8, hidden_dim_mlp_expert=512, moe_output_dim=1024, topk_experts=2):\n",
    "        \"\"\"\n",
    "        Multimodal Ensemble Model using Top-K Mixture of Experts for each modality.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): Number of output classes.\n",
    "            num_experts_per_modality (int): Total number of experts per MoE layer.\n",
    "            hidden_dim_mlp_expert (int): Hidden dimension for MLP experts.\n",
    "            moe_output_dim (int): Output dimension of each MoE layer.\n",
    "            topk_experts (int): Number of top experts to select. Defaults to 2.\n",
    "        \"\"\"\n",
    "        super(MultimodalEnsemble, self).__init__()\n",
    "\n",
    "        self.landsat_expert = LandsatExpert()\n",
    "        self.bioclim_expert = BioclimExpert()\n",
    "        self.sentinel_expert = SentinelExpert()\n",
    "\n",
    "        landsat_feat_dim = 512\n",
    "        bioclim_feat_dim = 512\n",
    "        sentinel_feat_dim = self.sentinel_expert.output_dim\n",
    "\n",
    "        self.landsat_moe = TopKMoELayer(\n",
    "            input_dim=landsat_feat_dim,\n",
    "            output_dim=moe_output_dim,\n",
    "            num_experts=num_experts_per_modality,\n",
    "            hidden_dim_mlp_expert=hidden_dim_mlp_expert,\n",
    "            k=topk_experts\n",
    "        )\n",
    "        self.bioclim_moe = TopKMoELayer(\n",
    "            input_dim=bioclim_feat_dim,\n",
    "            output_dim=moe_output_dim,\n",
    "            num_experts=num_experts_per_modality,\n",
    "            hidden_dim_mlp_expert=hidden_dim_mlp_expert,\n",
    "            k=topk_experts\n",
    "        )\n",
    "        self.sentinel_moe = TopKMoELayer(\n",
    "            input_dim=sentinel_feat_dim,\n",
    "            output_dim=moe_output_dim,\n",
    "            num_experts=num_experts_per_modality,\n",
    "            hidden_dim_mlp_expert=hidden_dim_mlp_expert,\n",
    "            k=topk_experts\n",
    "        )\n",
    "\n",
    "        fusion_input_dim = moe_output_dim * 3\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(fusion_input_dim),\n",
    "            nn.Linear(fusion_input_dim, fusion_input_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(fusion_input_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_landsat, x_bioclim, x_sentinel):\n",
    "        landsat_feat = self.landsat_expert(x_landsat)\n",
    "        bioclim_feat = self.bioclim_expert(x_bioclim)\n",
    "        sentinel_feat = self.sentinel_expert(x_sentinel)\n",
    "\n",
    "        landsat_moe_out = self.landsat_moe(landsat_feat)\n",
    "        bioclim_moe_out = self.bioclim_moe(bioclim_feat)\n",
    "        sentinel_moe_out = self.sentinel_moe(sentinel_feat)\n",
    "\n",
    "        fusion = torch.cat([landsat_moe_out, bioclim_moe_out, sentinel_moe_out], dim=1)\n",
    "\n",
    "        out = self.classifier(fusion)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T03:19:35.622882Z",
     "iopub.status.busy": "2025-04-21T03:19:35.622512Z",
     "iopub.status.idle": "2025-04-21T03:19:35.629947Z",
     "shell.execute_reply": "2025-04-21T03:19:35.628562Z",
     "shell.execute_reply.started": "2025-04-21T03:19:35.622852Z"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    # Set seed for Python's built-in random number generator\n",
    "    torch.manual_seed(seed)\n",
    "    # Set seed for numpy\n",
    "    np.random.seed(seed)\n",
    "    # Set seed for CUDA if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # Set cuDNN's random number generator seed for deterministic behavior\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:31.611823Z",
     "start_time": "2024-04-30T21:25:31.607373Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-21T03:19:38.429859Z",
     "iopub.status.busy": "2025-04-21T03:19:38.428972Z",
     "iopub.status.idle": "2025-04-21T03:19:40.105619Z",
     "shell.execute_reply": "2025-04-21T03:19:40.104480Z",
     "shell.execute_reply.started": "2025-04-21T03:19:38.429803Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = CUDA\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Check if cuda is available\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"DEVICE = CUDA\")\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "num_classes = 11255 # Number of all unique classes within the PO and PA data.\n",
    "model = MultimodalEnsemble(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Nothing special, just a standard Pytorch training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:32.181927Z",
     "start_time": "2024-04-30T21:25:32.177073Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-21T03:19:41.747669Z",
     "iopub.status.busy": "2025-04-21T03:19:41.747298Z",
     "iopub.status.idle": "2025-04-21T03:19:41.755310Z",
     "shell.execute_reply": "2025-04-21T03:19:41.754245Z",
     "shell.execute_reply.started": "2025-04-21T03:19:41.747640Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.00025\n",
    "num_epochs = 15\n",
    "positive_weigh_factor = 1.0\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=25, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T03:19:42.478718Z",
     "iopub.status.busy": "2025-04-21T03:19:42.477716Z",
     "iopub.status.idle": "2025-04-21T03:19:42.485998Z",
     "shell.execute_reply": "2025-04-21T03:19:42.484893Z",
     "shell.execute_reply.started": "2025-04-21T03:19:42.478664Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def f1_score_multilabel(preds, targets):\n",
    "    \"\"\"\n",
    "    計算 multi-label 的 F1 score (sample wise 平均) 並回傳 recall 和 precision\n",
    "    \n",
    "    Args:\n",
    "        preds (torch.Tensor): 預測值，形狀為 (batch_size, num_classes)，整數類型\n",
    "        targets (torch.Tensor): 實際標籤，形狀為 (batch_size, num_classes)，整數類型\n",
    "        \n",
    "    Returns:\n",
    "        tuple: sample wise 平均的 F1 score, recall, precision\n",
    "    \"\"\"\n",
    "\n",
    "    preds = preds.float()\n",
    "    targets = targets.float()\n",
    "    \n",
    "    # 計算 TP, FP, FN\n",
    "    tp = (preds * targets).sum(dim=1).float()\n",
    "    fp = (preds * (1 - targets)).sum(dim=1).float()\n",
    "    fn = ((1 - preds) * targets).sum(dim=1).float()\n",
    "    \n",
    "    # 計算 precision 和 recall\n",
    "    precision = tp / (tp + fp + 1e-7)\n",
    "    recall = tp / (tp + fn + 1e-7)\n",
    "    \n",
    "    # 計算 F1 score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-7)\n",
    "    \n",
    "    # 計算 sample wise 平均的 F1 score, recall, precision\n",
    "    average_f1 = f1.mean().item()\n",
    "    average_precision = precision.mean().item()\n",
    "    average_recall = recall.mean().item()\n",
    "    \n",
    "    return average_f1, average_precision, average_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-30T21:25:34.536634Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-21T03:24:09.329399Z",
     "iopub.status.busy": "2025-04-21T03:24:09.325757Z"
    },
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(f\"Training for {num_epochs} epochs started.\")\n",
    "# from tqdm import tqdm\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# thresholds = np.arange(0.1, 0.99, 0.05)\n",
    "\n",
    "\n",
    "# best_f1_overall = 0\n",
    "# best_model_state = None\n",
    "# best_epoch = 0\n",
    "# best_threshold_overall = None\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     val_f1_scores = {th: [] for th in thresholds}\n",
    "\n",
    "#     for batch_idx, (data1, data2, data3, targets, _) in enumerate(tqdm(train_loader)):\n",
    "#         data1, data2, data3, targets = data1.to(device), data2.to(device), data3.to(device), targets.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(data1, data2, data3)\n",
    "\n",
    "#         pos_weight = targets * positive_weigh_factor\n",
    "#         criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "#         loss = criterion(outputs, targets)\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "\n",
    "#         for threshold in thresholds:\n",
    "#             predictions = (outputs.sigmoid() > threshold).cpu()\n",
    "#             f1, _, _ = f1_score_multilabel(predictions, (targets == 1).cpu())\n",
    "#             val_f1_scores[threshold].append(f1)\n",
    "\n",
    "#         if batch_idx % 278 == 0:\n",
    "#             print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "#     # Tính trung bình F1 cho mỗi threshold\n",
    "#     best_threshold, best_f1 = max(\n",
    "#         [(th, np.mean(f1s)) for th, f1s in val_f1_scores.items()],\n",
    "#         key=lambda x: x[1]\n",
    "#     )\n",
    "\n",
    "#     tqdm.write(f\"Epoch {epoch+1}. Validation best F1 score: {best_f1:.5f} with threshold: {best_threshold:.3f}\")\n",
    "#     print(\"Scheduler:\", scheduler.state_dict())\n",
    "\n",
    "#     # Nếu F1 tốt hơn best_f1_overall thì lưu model\n",
    "#     if best_f1 > best_f1_overall:\n",
    "#         best_f1_overall = best_f1\n",
    "#         best_model_state = model.state_dict()\n",
    "#         best_epoch = epoch + 1\n",
    "#         best_threshold_overall = best_threshold\n",
    "\n",
    "# # Save the best model sau tất cả các epoch\n",
    "# if best_model_state is not None:\n",
    "#     torch.save(best_model_state, f\"best_multimodal_model_epoch_moe_routing-top-2_12_expert{best_epoch}_f1_{best_f1_overall:.4f}.pth\")\n",
    "#     print(f\"Best model saved from epoch {best_epoch} with F1 {best_f1_overall:.5f} at threshold {best_threshold_overall:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Your Definitions ---\n",
    "# # Check if cuda is available\n",
    "# device = torch.device(\"cpu\")\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     print(f\"DEVICE = {device}\")\n",
    "# else:\n",
    "#      print(f\"DEVICE = {device}\")\n",
    "\n",
    "# random.seed(42) # Set random seed for reproducibility\n",
    "\n",
    "# num_classes = 11255 # Number of all unique classes within the PO and PA data.l\n",
    "\n",
    "# earning_rate = 0.00025\n",
    "# num_epochs = 15\n",
    "# positive_weigh_factor = 1.0 # Note: This will be applied per-target where target is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR # Import your specific scheduler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import random # Import random for setting seed\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "# Assume MultimodalEnsemble class is defined elsewhere\n",
    "# class MultimodalEnsemble(nn.Module):\n",
    "#     def __init__(self, num_classes): ...\n",
    "#     def forward(self, data1, data2, data3): ...\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "\n",
    "# --- Assume these are defined elsewhere ---\n",
    "# class YourDataset(Dataset):\n",
    "#     def __init__(self, ...): ...\n",
    "#     def __len__(self): ...\n",
    "#     def __getitem__(self, idx): # Should return data1, data2, data3, targets, _\n",
    "# full_dataset = YourDataset(...) # Your entire dataset instance\n",
    "\n",
    "# def f1_score_multilabel(preds, targets):\n",
    "#    # This function should calculate multilabel F1.\n",
    "#    # It should handle cases where targets are all 0s or all 1s gracefully (e.g., return 0 or NaN).\n",
    "#    # It MUST expect 'preds' and 'targets' to be PyTorch Tensors (e.g., boolean or float 0/1).\n",
    "#    # Example (using sklearn, demonstrating how to handle tensors):\n",
    "#    # from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "#    # # Convert tensors to numpy arrays *inside* the function if sklearn is used\n",
    "#    # preds_np = preds.cpu().numpy()\n",
    "#    # targets_np = targets.cpu().numpy() # If targets were originally on device\n",
    "#    # # Or if inputs are already cpu tensors, just .numpy()\n",
    "#    # preds_np = preds.numpy()\n",
    "#    # targets_np = targets.numpy()\n",
    "#    # f1 = f1_score(targets_np, preds_np, average='macro', zero_division=0) # or 'weighted' or 'samples'\n",
    "#    # precision = precision_score(targets_np, preds_np, average='macro', zero_division=0)\n",
    "#    # recall = recall_score(targets_np, preds_np, average='macro', zero_division=0)\n",
    "#    # return f1, precision, recall\n",
    "#    pass # Replace with your actual implementation\n",
    "\n",
    "# batch_size = ... # Define your batch size\n",
    "# num_workers = ... # Define your num_workers for DataLoaders\n",
    "# ------------------------------------------\n",
    "\n",
    "# # Cross-validation parameters\n",
    "# n_splits = 5 # You can change the number of folds\n",
    "# kf = KFold(n_splits=n_splits, shuffle=True, random_state=42) # Shuffle for better data distribution\n",
    "\n",
    "# thresholds = np.arange(0.1, 0.99, 0.05)\n",
    "\n",
    "# # Overall best tracking across all folds and epochs\n",
    "# best_f1_overall = -1.0 # Initialize with a low value\n",
    "# best_model_state = None\n",
    "# best_epoch_overall = -1\n",
    "# best_fold_overall = -1\n",
    "# best_threshold_overall = None\n",
    "\n",
    "# print(f\"Starting {n_splits}-fold cross-validation.\")\n",
    "\n",
    "# # --- Start K-Fold Loop ---\n",
    "# # kf.split returns indices for training and validation sets for each fold\n",
    "# for fold, (train_index, val_index) in enumerate(kf.split(dataset_alpine)):\n",
    "#     print(f\"\\n--- Fold {fold+1}/{n_splits} ---\")\n",
    "\n",
    "#     # Create data subsets and loaders for the current fold\n",
    "#     train_dataset = Subset(dataset_alpine, train_index)\n",
    "#     val_dataset = Subset(dataset_alpine, val_index)\n",
    "\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=3)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=3) # No need to shuffle validation data\n",
    "\n",
    "#     # Re-initialize model, optimizer, scheduler for each fold\n",
    "#     # This ensures each fold starts with the same initial conditions\n",
    "#     print(f\"Initializing model, optimizer, and scheduler for Fold {fold+1}...\")\n",
    "#     model = MultimodalEnsemble(num_classes) # Instantiate your model\n",
    "#     model.to(device)\n",
    "\n",
    "#     # Re-initialize optimizer for the new model parameters\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate) # Instantiate your optimizer\n",
    "\n",
    "#     # Re-initialize scheduler\n",
    "#     # Note: Your original code stepped scheduler per batch with T_max=25.\n",
    "#     # CosineAnnealingLR with T_max usually relates to epochs or total steps.\n",
    "#     # Keeping the per-batch step here to match your original code structure,\n",
    "#     # but standard usage might step per epoch with T_max = num_epochs.\n",
    "#     # If T_max=25 is meant to be total steps over *all* epochs for the current fold,\n",
    "#     # you'd calculate it as `len(train_loader) * num_epochs`.\n",
    "#     # Assuming T_max=25 was intentional based on your original code's stepping.\n",
    "#     scheduler = CosineAnnealingLR(optimizer, T_max=25, verbose=True) # Instantiate your scheduler with your T_max\n",
    "\n",
    "#     # --- Start Epoch Loop for the current fold ---\n",
    "#     print(f\"Training for {num_epochs} epochs for Fold {fold+1} started.\")\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # --- Training Phase ---\n",
    "#         model.train()\n",
    "#         running_train_loss = 0.0\n",
    "\n",
    "#         print(f\"Fold {fold+1}, Epoch {epoch+1}/{num_epochs} - Training\")\n",
    "#         train_pbar = tqdm(train_loader, desc=f\"Fold {fold+1} Epoch {epoch+1} [Train]\", leave=False)\n",
    "\n",
    "#         for batch_idx, (data1, data2, data3, targets, _) in enumerate(train_pbar):\n",
    "#             data1, data2, data3, targets = data1.to(device), data2.to(device), data3.to(device), targets.to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(data1, data2, data3)\n",
    "\n",
    "#             # Criterion creation inside batch loop using scalar pos_weight\n",
    "#             # BCEWithLogitsLoss expects targets to be float\n",
    "#             criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor(positive_weigh_factor, device=device))\n",
    "#             loss = criterion(outputs, targets.float())\n",
    "\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # Scheduler step *after* optimizer step (per batch based on your original code's behavior)\n",
    "#             scheduler.step()\n",
    "\n",
    "#             running_train_loss += loss.item()\n",
    "#             train_pbar.set_postfix({'loss': loss.item()}) # Update progress bar\n",
    "\n",
    "\n",
    "#         # Print average training loss for the epoch\n",
    "#         avg_train_loss = running_train_loss / len(train_loader)\n",
    "#         print(f\"Fold {fold+1}, Epoch {epoch+1} - Avg Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "#         # --- Validation Phase (after training epoch) ---\n",
    "#         model.eval() # Set model to evaluation mode\n",
    "#         print(f\"Fold {fold+1}, Epoch {epoch+1} - Validating\")\n",
    "#         val_preds_sigmoid = [] # Store sigmoid outputs\n",
    "#         val_targets_list = []\n",
    "#         running_val_loss = 0.0\n",
    "\n",
    "#         # Use torch.no_grad() during validation to save memory and computation\n",
    "#         with torch.no_grad():\n",
    "#             val_pbar = tqdm(val_loader, desc=f\"Fold {fold+1} Epoch {epoch+1} [Val]\", leave=False)\n",
    "#             for data1, data2, data3, targets, _ in val_pbar:\n",
    "#                 data1, data2, data3, targets = data1.to(device), data2.to(device), data3.to(device), targets.to(device)\n",
    "\n",
    "#                 outputs = model(data1, data2, data3)\n",
    "\n",
    "#                 # Calculate validation loss (optional) - typically use a standard BCEWithLogitsLoss without pos_weight for validation\n",
    "#                 val_criterion = torch.nn.BCEWithLogitsLoss()\n",
    "#                 running_val_loss += val_criterion(outputs, targets.float()).item()\n",
    "\n",
    "#                 # Collect predictions (sigmoid outputs) and targets for metric calculation\n",
    "#                 # Move to CPU before appending - Keep as Tensors!\n",
    "#                 val_preds_sigmoid.append(outputs.sigmoid().cpu())\n",
    "#                 val_targets_list.append(targets.cpu()) # Keep as Tensors!\n",
    "\n",
    "#         # Calculate average validation loss\n",
    "#         avg_val_loss = running_val_loss / len(val_loader)\n",
    "#         print(f\"Fold {fold+1}, Epoch {epoch+1} - Avg Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "#         # --- Threshold Tuning on the entire Validation Set ---\n",
    "#         # Concatenate all batch results - these are now CPU PyTorch Tensors\n",
    "#         all_val_preds_sigmoid = torch.cat(val_preds_sigmoid)\n",
    "#         all_val_targets = torch.cat(val_targets_list) # Targets are 0/1 integer tensors\n",
    "\n",
    "#         epoch_best_f1_val = -1.0\n",
    "#         epoch_best_threshold_val = -1.0\n",
    "\n",
    "#         # Prepare targets for f1_score_multilabel - Keep as PyTorch Tensors!\n",
    "#         # Assuming f1_score_multilabel expects boolean or float PyTorch tensors\n",
    "#         formatted_val_targets_tensor = (all_val_targets == 1) # Boolean tensor, or all_val_targets.float()\n",
    "\n",
    "#         print(f\"Fold {fold+1}, Epoch {epoch+1} - Tuning thresholds on validation set...\")\n",
    "#         # Use a smaller loop for tqdm here as threshold loop is fast\n",
    "#         threshold_pbar = tqdm(thresholds, desc=\"Tuning Thresholds\", leave=False)\n",
    "#         for threshold in threshold_pbar:\n",
    "#             # Apply threshold to validation predictions - Keep as PyTorch Tensors!\n",
    "#             val_predictions_thresholded_tensor = (all_val_preds_sigmoid > threshold) # Boolean tensor\n",
    "\n",
    "#             # Calculate F1 for the current threshold\n",
    "#             # Pass the PyTorch tensors directly\n",
    "#             current_f1_val, _, _ = f1_score_multilabel(val_predictions_thresholded_tensor, formatted_val_targets_tensor)\n",
    "\n",
    "#             # Update best threshold for the current epoch's validation\n",
    "#             if current_f1_val > epoch_best_f1_val:\n",
    "#                 epoch_best_f1_val = current_f1_val\n",
    "#                 epoch_best_threshold_val = threshold\n",
    "#                 # Update pbar postfix with the best F1 found so far in this epoch\n",
    "#                 threshold_pbar.set_postfix({'best_f1_epoch': epoch_best_f1_val, 'threshold': epoch_best_threshold_val})\n",
    "\n",
    "\n",
    "#         tqdm.write(f\"Fold {fold+1}, Epoch {epoch+1}. Validation best F1: {epoch_best_f1_val:.5f} with threshold: {epoch_best_threshold_val:.3f}\")\n",
    "#         # print(\"Scheduler:\", scheduler.state_dict()) # Optional: print scheduler state\n",
    "\n",
    "#         # --- Update Overall Best Model found across all folds and epochs ---\n",
    "#         if epoch_best_f1_val > best_f1_overall:\n",
    "#             best_f1_overall = epoch_best_f1_val\n",
    "#             # Save model state\n",
    "#             best_model_state = model.state_dict()\n",
    "#             best_epoch_overall = epoch + 1\n",
    "#             best_fold_overall = fold + 1\n",
    "#             best_threshold_overall = epoch_best_threshold_val\n",
    "#             print(f\"*** New overall best F1 found: {best_f1_overall:.5f} (Fold {best_fold_overall}, Epoch {best_epoch_overall}) ***\")\n",
    "\n",
    "#         # Optional: Add early stopping logic here based on epoch_best_f1_val\n",
    "\n",
    "#     # --- End Epoch Loop for the current fold ---\n",
    "#     # Optional: Save best model for this fold if desired\n",
    "#     # (e.g., torch.save(model.state_dict(), f\"fold_{fold+1}_best_model_f1_{best_f1_this_fold:.4f}.pth\"))\n",
    "\n",
    "\n",
    "# # --- End K-Fold Loop ---\n",
    "\n",
    "# # Save the overall best model found across all folds and epochs\n",
    "# if best_model_state is not None:\n",
    "#     # Construct a descriptive filename\n",
    "#     save_path = f\"overall_best_multimodal_model_fold{best_fold_overall}_epoch{best_epoch_overall}_f1_{best_f1_overall:.4f}.pth\"\n",
    "#     torch.save(best_model_state, save_path)\n",
    "#     print(f\"\\n--- Cross-validation finished ---\")\n",
    "#     print(f\"Overall best model saved from Fold {best_fold_overall}, Epoch {best_epoch_overall} with F1 {best_f1_overall:.5f} at threshold {best_threshold_overall:.3f}\")\n",
    "# else:\n",
    "#     print(\"\\n--- Cross-validation finished ---\")\n",
    "#     print(\"No model was saved. This might happen if training failed or validation F1 never improved above initial -1.0.\")\n",
    "\n",
    "# # --- Optional: Evaluate the overall best model on a separate test set ---\n",
    "# # (Same logic as before, but ensure tensors are passed to f1_score_multilabel)\n",
    "# # print(\"\\nEvaluating overall best model on test set (if available)...\")\n",
    "# # if best_model_state is not None and 'test_loader' in locals(): # Assuming you have a test_loader defined elsewhere\n",
    "# #     test_model = MultimodalEnsemble(num_classes) # Initialize a new model instance\n",
    "# #     test_model.load_state_dict(best_model_state)\n",
    "# #     test_model.to(device)\n",
    "# #     test_model.eval()\n",
    "# #     test_preds_sigmoid = []\n",
    "# #     test_targets_list = []\n",
    "# #     with torch.no_grad():\n",
    "# #         for data1, data2, data3, targets, _ in tqdm(test_loader, desc=\"Evaluating on Test Set\"):\n",
    "# #             data1, data2, data3, targets = data1.to(device), data2.to(device), data3.to(device), targets.to(device)\n",
    "# #             outputs = test_model(data1, data2, data3)\n",
    "# #             test_preds_sigmoid.append(outputs.sigmoid().cpu()) # Keep as Tensor\n",
    "# #             test_targets_list.append(targets.cpu()) # Keep as Tensor\n",
    "\n",
    "# #     all_test_preds_sigmoid = torch.cat(test_preds_sigmoid)\n",
    "# #     all_test_targets = torch.cat(test_targets_list)\n",
    "\n",
    "# #     # Apply the best threshold found during CV validation - Keep as Tensor!\n",
    "# #     final_test_predictions_tensor = (all_test_preds_sigmoid > best_threshold_overall)\n",
    "\n",
    "# #     # Prepare test targets - Keep as Tensor!\n",
    "# #     final_test_targets_tensor = (all_test_targets == 1)\n",
    "\n",
    "# #     # Pass Tensors to f1_score_multilabel\n",
    "# #     final_test_f1, _, _ = f1_score_multilabel(final_test_predictions_tensor, final_test_targets_tensor)\n",
    "# #     print(f\"Final Test Set F1 using overall best threshold ({best_threshold_overall:.3f}): {final_test_f1:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: saved_fold_models_2\n",
      "Starting 5-fold cross-validation for ensembling.\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "Initializing model, optimizer, and scheduler for Fold 1...\n",
      "Training for 15 epochs for Fold 1 started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 1 - Avg Training Loss: 0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 1 - Avg Validation Loss: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 1. Validation best F1 for epoch: 0.28898 with threshold: 0.150\n",
      "Fold 1: New best validation F1 for this fold: 0.28898 at Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 2 - Avg Training Loss: 0.0049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 2 - Avg Validation Loss: 0.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 2. Validation best F1 for epoch: 0.34371 with threshold: 0.150\n",
      "Fold 1: New best validation F1 for this fold: 0.34371 at Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 3 - Avg Training Loss: 0.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 3 - Avg Validation Loss: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 3. Validation best F1 for epoch: 0.36371 with threshold: 0.150\n",
      "Fold 1: New best validation F1 for this fold: 0.36371 at Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 4 - Avg Training Loss: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 4 - Avg Validation Loss: 0.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 4. Validation best F1 for epoch: 0.37666 with threshold: 0.200\n",
      "Fold 1: New best validation F1 for this fold: 0.37666 at Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 5 - Avg Training Loss: 0.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 5 - Avg Validation Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 5. Validation best F1 for epoch: 0.39196 with threshold: 0.200\n",
      "Fold 1: New best validation F1 for this fold: 0.39196 at Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 6 - Avg Training Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 6 - Avg Validation Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 6. Validation best F1 for epoch: 0.38658 with threshold: 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 7 - Avg Training Loss: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 7 - Avg Validation Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 7. Validation best F1 for epoch: 0.38817 with threshold: 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 8 - Avg Training Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 8 - Avg Validation Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 8. Validation best F1 for epoch: 0.38717 with threshold: 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 9 - Avg Training Loss: 0.0039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 9 - Avg Validation Loss: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 9. Validation best F1 for epoch: 0.39903 with threshold: 0.200\n",
      "Fold 1: New best validation F1 for this fold: 0.39903 at Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 10 - Avg Training Loss: 0.0039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 10 - Avg Validation Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 10. Validation best F1 for epoch: 0.41116 with threshold: 0.200\n",
      "Fold 1: New best validation F1 for this fold: 0.41116 at Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 11 - Avg Training Loss: 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 11 - Avg Validation Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 11. Validation best F1 for epoch: 0.41581 with threshold: 0.200\n",
      "Fold 1: New best validation F1 for this fold: 0.41581 at Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 12 - Avg Training Loss: 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 12 - Avg Validation Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 12. Validation best F1 for epoch: 0.41647 with threshold: 0.200\n",
      "Fold 1: New best validation F1 for this fold: 0.41647 at Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 13 - Avg Training Loss: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 13 - Avg Validation Loss: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 13. Validation best F1 for epoch: 0.41182 with threshold: 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 14 - Avg Training Loss: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 14 - Avg Validation Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 14. Validation best F1 for epoch: 0.40775 with threshold: 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 15 - Avg Training Loss: 0.0036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 15 - Avg Validation Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 15. Validation best F1 for epoch: 0.40687 with threshold: 0.200\n",
      "Saved best model for Fold 1 (F1: 0.41647, Threshold: 0.200) to saved_fold_models_2/fold_1_epoch_best_f1_0.4165_thresh_0.200.pth\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "Initializing model, optimizer, and scheduler for Fold 2...\n",
      "Training for 15 epochs for Fold 2 started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 1 - Avg Training Loss: 0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 1 - Avg Validation Loss: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 1. Validation best F1 for epoch: 0.28541 with threshold: 0.150\n",
      "Fold 2: New best validation F1 for this fold: 0.28541 at Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 2 - Avg Training Loss: 0.0049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 2 - Avg Validation Loss: 0.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 2. Validation best F1 for epoch: 0.34224 with threshold: 0.200\n",
      "Fold 2: New best validation F1 for this fold: 0.34224 at Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 3 - Avg Training Loss: 0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 3 - Avg Validation Loss: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 3. Validation best F1 for epoch: 0.36944 with threshold: 0.200\n",
      "Fold 2: New best validation F1 for this fold: 0.36944 at Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 4 - Avg Training Loss: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 4 - Avg Validation Loss: 0.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 4. Validation best F1 for epoch: 0.37854 with threshold: 0.200\n",
      "Fold 2: New best validation F1 for this fold: 0.37854 at Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 5 - Avg Training Loss: 0.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 5 - Avg Validation Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 5. Validation best F1 for epoch: 0.39308 with threshold: 0.200\n",
      "Fold 2: New best validation F1 for this fold: 0.39308 at Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 6 - Avg Training Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 6 - Avg Validation Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 6. Validation best F1 for epoch: 0.38865 with threshold: 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 7 - Avg Training Loss: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 7 - Avg Validation Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 7. Validation best F1 for epoch: 0.39381 with threshold: 0.200\n",
      "Fold 2: New best validation F1 for this fold: 0.39381 at Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 8 - Avg Training Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 8 - Avg Validation Loss: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 8. Validation best F1 for epoch: 0.37263 with threshold: 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 9 - Avg Training Loss: 0.0039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 9 - Avg Validation Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 9. Validation best F1 for epoch: 0.38444 with threshold: 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 10 - Avg Training Loss: 0.0039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 10 - Avg Validation Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 10. Validation best F1 for epoch: 0.40961 with threshold: 0.200\n",
      "Fold 2: New best validation F1 for this fold: 0.40961 at Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 11 - Avg Training Loss: 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 11 - Avg Validation Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 11. Validation best F1 for epoch: 0.41496 with threshold: 0.200\n",
      "Fold 2: New best validation F1 for this fold: 0.41496 at Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 12 - Avg Training Loss: 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 12 - Avg Validation Loss: 0.0039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 12. Validation best F1 for epoch: 0.42138 with threshold: 0.200\n",
      "Fold 2: New best validation F1 for this fold: 0.42138 at Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 13 - Avg Training Loss: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 13 - Avg Validation Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 13. Validation best F1 for epoch: 0.42134 with threshold: 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 14 - Avg Training Loss: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 14 - Avg Validation Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 14. Validation best F1 for epoch: 0.41728 with threshold: 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 15 - Avg Training Loss: 0.0036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 15 - Avg Validation Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 15. Validation best F1 for epoch: 0.41809 with threshold: 0.200\n",
      "Saved best model for Fold 2 (F1: 0.42138, Threshold: 0.200) to saved_fold_models_2/fold_2_epoch_best_f1_0.4214_thresh_0.200.pth\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "Initializing model, optimizer, and scheduler for Fold 3...\n",
      "Training for 15 epochs for Fold 3 started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 1 - Avg Training Loss: 0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 1 - Avg Validation Loss: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 1. Validation best F1 for epoch: 0.29539 with threshold: 0.150\n",
      "Fold 3: New best validation F1 for this fold: 0.29539 at Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 2 - Avg Training Loss: 0.0049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 2 - Avg Validation Loss: 0.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 2. Validation best F1 for epoch: 0.34296 with threshold: 0.150\n",
      "Fold 3: New best validation F1 for this fold: 0.34296 at Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 3 - Avg Training Loss: 0.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 3 - Avg Validation Loss: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 3. Validation best F1 for epoch: 0.36326 with threshold: 0.200\n",
      "Fold 3: New best validation F1 for this fold: 0.36326 at Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 4 - Avg Training Loss: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 4 - Avg Validation Loss: 0.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 4. Validation best F1 for epoch: 0.37635 with threshold: 0.200\n",
      "Fold 3: New best validation F1 for this fold: 0.37635 at Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 5 - Avg Training Loss: 0.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 5 - Avg Validation Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 5. Validation best F1 for epoch: 0.39057 with threshold: 0.200\n",
      "Fold 3: New best validation F1 for this fold: 0.39057 at Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 6 - Avg Training Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 6 - Avg Validation Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 6. Validation best F1 for epoch: 0.38777 with threshold: 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 7 - Avg Training Loss: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 7 - Avg Validation Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 7. Validation best F1 for epoch: 0.38291 with threshold: 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 8 - Avg Training Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 8 - Avg Validation Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 8. Validation best F1 for epoch: 0.38379 with threshold: 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 9 - Avg Training Loss: 0.0039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 9 - Avg Validation Loss: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 9. Validation best F1 for epoch: 0.39880 with threshold: 0.200\n",
      "Fold 3: New best validation F1 for this fold: 0.39880 at Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 10 - Avg Training Loss: 0.0039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 10 - Avg Validation Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 10. Validation best F1 for epoch: 0.41135 with threshold: 0.200\n",
      "Fold 3: New best validation F1 for this fold: 0.41135 at Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 11 - Avg Training Loss: 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 11 - Avg Validation Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 11. Validation best F1 for epoch: 0.41337 with threshold: 0.200\n",
      "Fold 3: New best validation F1 for this fold: 0.41337 at Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 12 - Avg Training Loss: 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 12 - Avg Validation Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 12. Validation best F1 for epoch: 0.41996 with threshold: 0.200\n",
      "Fold 3: New best validation F1 for this fold: 0.41996 at Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 13 - Avg Training Loss: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 13 - Avg Validation Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 13. Validation best F1 for epoch: 0.42222 with threshold: 0.200\n",
      "Fold 3: New best validation F1 for this fold: 0.42222 at Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 14 - Avg Training Loss: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 14 - Avg Validation Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 14. Validation best F1 for epoch: 0.41256 with threshold: 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 15 - Avg Training Loss: 0.0036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 15 - Avg Validation Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 15. Validation best F1 for epoch: 0.41343 with threshold: 0.200\n",
      "Saved best model for Fold 3 (F1: 0.42222, Threshold: 0.200) to saved_fold_models_2/fold_3_epoch_best_f1_0.4222_thresh_0.200.pth\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "Initializing model, optimizer, and scheduler for Fold 4...\n",
      "Training for 15 epochs for Fold 4 started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 1 - Avg Training Loss: 0.0111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 1 - Avg Validation Loss: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 1. Validation best F1 for epoch: 0.29022 with threshold: 0.150\n",
      "Fold 4: New best validation F1 for this fold: 0.29022 at Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 2 - Avg Training Loss: 0.0049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 2 - Avg Validation Loss: 0.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 2. Validation best F1 for epoch: 0.33514 with threshold: 0.200\n",
      "Fold 4: New best validation F1 for this fold: 0.33514 at Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 3 - Avg Training Loss: 0.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 3 - Avg Validation Loss: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 3. Validation best F1 for epoch: 0.36545 with threshold: 0.200\n",
      "Fold 4: New best validation F1 for this fold: 0.36545 at Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 4 - Avg Training Loss: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 4 - Avg Validation Loss: 0.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 4. Validation best F1 for epoch: 0.37895 with threshold: 0.200\n",
      "Fold 4: New best validation F1 for this fold: 0.37895 at Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 5 - Avg Training Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 5 - Avg Validation Loss: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 5. Validation best F1 for epoch: 0.39520 with threshold: 0.200\n",
      "Fold 4: New best validation F1 for this fold: 0.39520 at Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 6 - Avg Training Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 6 - Avg Validation Loss: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 6. Validation best F1 for epoch: 0.39459 with threshold: 0.150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 7 - Avg Training Loss: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 7 - Avg Validation Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 7. Validation best F1 for epoch: 0.38706 with threshold: 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 8 - Avg Training Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 8 - Avg Validation Loss: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 8. Validation best F1 for epoch: 0.38706 with threshold: 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 9 - Avg Training Loss: 0.0039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 9 - Avg Validation Loss: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 9. Validation best F1 for epoch: 0.40134 with threshold: 0.200\n",
      "Fold 4: New best validation F1 for this fold: 0.40134 at Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 10 - Avg Training Loss: 0.0039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 10 - Avg Validation Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 10. Validation best F1 for epoch: 0.40932 with threshold: 0.200\n",
      "Fold 4: New best validation F1 for this fold: 0.40932 at Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 11 - Avg Training Loss: 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 11 - Avg Validation Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 11. Validation best F1 for epoch: 0.41618 with threshold: 0.200\n",
      "Fold 4: New best validation F1 for this fold: 0.41618 at Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 12 - Avg Training Loss: 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 12 - Avg Validation Loss: 0.0039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 12. Validation best F1 for epoch: 0.42163 with threshold: 0.200\n",
      "Fold 4: New best validation F1 for this fold: 0.42163 at Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 13 - Avg Training Loss: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 13 - Avg Validation Loss: 0.0039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 13. Validation best F1 for epoch: 0.42027 with threshold: 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 14 - Avg Training Loss: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 14 - Avg Validation Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 14. Validation best F1 for epoch: 0.41533 with threshold: 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 15 - Avg Training Loss: 0.0036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 15 - Avg Validation Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 15. Validation best F1 for epoch: 0.41731 with threshold: 0.250\n",
      "Saved best model for Fold 4 (F1: 0.42163, Threshold: 0.200) to saved_fold_models_2/fold_4_epoch_best_f1_0.4216_thresh_0.200.pth\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "Initializing model, optimizer, and scheduler for Fold 5...\n",
      "Training for 15 epochs for Fold 5 started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 1 - Avg Training Loss: 0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 1 - Avg Validation Loss: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 1. Validation best F1 for epoch: 0.29119 with threshold: 0.150\n",
      "Fold 5: New best validation F1 for this fold: 0.29119 at Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 2 - Avg Training Loss: 0.0049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 2 - Avg Validation Loss: 0.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 2. Validation best F1 for epoch: 0.34541 with threshold: 0.150\n",
      "Fold 5: New best validation F1 for this fold: 0.34541 at Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 3 - Avg Training Loss: 0.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 3 - Avg Validation Loss: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 3. Validation best F1 for epoch: 0.37024 with threshold: 0.200\n",
      "Fold 5: New best validation F1 for this fold: 0.37024 at Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 4 - Avg Training Loss: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 4 - Avg Validation Loss: 0.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 4. Validation best F1 for epoch: 0.38279 with threshold: 0.200\n",
      "Fold 5: New best validation F1 for this fold: 0.38279 at Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 5 - Avg Training Loss: 0.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 5 - Avg Validation Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 5. Validation best F1 for epoch: 0.39307 with threshold: 0.200\n",
      "Fold 5: New best validation F1 for this fold: 0.39307 at Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 6 - Avg Training Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 6 - Avg Validation Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 6. Validation best F1 for epoch: 0.38478 with threshold: 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 7 - Avg Training Loss: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 7 - Avg Validation Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 7. Validation best F1 for epoch: 0.39203 with threshold: 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 8 - Avg Training Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 8 - Avg Validation Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 8. Validation best F1 for epoch: 0.39508 with threshold: 0.200\n",
      "Fold 5: New best validation F1 for this fold: 0.39508 at Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 9 - Avg Training Loss: 0.0039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 9 - Avg Validation Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 9. Validation best F1 for epoch: 0.39406 with threshold: 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 10 - Avg Training Loss: 0.0039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 10 - Avg Validation Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 10. Validation best F1 for epoch: 0.40976 with threshold: 0.200\n",
      "Fold 5: New best validation F1 for this fold: 0.40976 at Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 11 - Avg Training Loss: 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 11 - Avg Validation Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 11. Validation best F1 for epoch: 0.41584 with threshold: 0.200\n",
      "Fold 5: New best validation F1 for this fold: 0.41584 at Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 12 - Avg Training Loss: 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 12 - Avg Validation Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 12. Validation best F1 for epoch: 0.42156 with threshold: 0.200\n",
      "Fold 5: New best validation F1 for this fold: 0.42156 at Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 13 - Avg Training Loss: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 13 - Avg Validation Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 13. Validation best F1 for epoch: 0.42445 with threshold: 0.200\n",
      "Fold 5: New best validation F1 for this fold: 0.42445 at Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 14 - Avg Training Loss: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 14 - Avg Validation Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 14. Validation best F1 for epoch: 0.41545 with threshold: 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 15 - Avg Training Loss: 0.0036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 15 - Avg Validation Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 15. Validation best F1 for epoch: 0.41184 with threshold: 0.200\n",
      "Saved best model for Fold 5 (F1: 0.42445, Threshold: 0.200) to saved_fold_models_2/fold_5_epoch_best_f1_0.4244_thresh_0.200.pth\n",
      "\n",
      "--- Cross-validation finished ---\n",
      "Training complete for 5 folds.\n",
      "Saved models for ensembling (best per fold):\n",
      "saved_fold_models_2/fold_1_epoch_best_f1_0.4165_thresh_0.200.pth\n",
      "saved_fold_models_2/fold_2_epoch_best_f1_0.4214_thresh_0.200.pth\n",
      "saved_fold_models_2/fold_3_epoch_best_f1_0.4222_thresh_0.200.pth\n",
      "saved_fold_models_2/fold_4_epoch_best_f1_0.4216_thresh_0.200.pth\n",
      "saved_fold_models_2/fold_5_epoch_best_f1_0.4244_thresh_0.200.pth\n",
      "\n",
      "--- Example: How to load saved models for ensembling ---\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation parameters\n",
    "n_splits = 5 # You can change the number of folds\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42) # Shuffle for better data distribution\n",
    "\n",
    "thresholds = np.arange(0.1, 0.99, 0.05) # Thresholds to tune\n",
    "\n",
    "# List to store paths of saved models for ensembling later\n",
    "saved_model_paths = []\n",
    "saved_thresholds = [] # Optional: store the best threshold found for each fold\n",
    "\n",
    "# Create a directory to save models if it doesn't exist\n",
    "models_dir = \"saved_fold_models_2\"\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "    print(f\"Created directory: {models_dir}\")\n",
    "\n",
    "\n",
    "print(f\"Starting {n_splits}-fold cross-validation for ensembling.\")\n",
    "\n",
    "# --- Start K-Fold Loop ---\n",
    "# kf.split returns indices for training and validation sets for each fold\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(dataset_alpine)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{n_splits} ---\")\n",
    "\n",
    "    # Create data subsets and loaders for the current fold\n",
    "    train_dataset = Subset(dataset_alpine, train_index)\n",
    "    val_dataset = Subset(dataset_alpine, val_index)\n",
    "\n",
    "    # Use num_workers if needed, set based on your system\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=3)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=3) # No need to shuffle validation data\n",
    "\n",
    "    # Re-initialize model, optimizer, scheduler for each fold\n",
    "    # This ensures each fold starts with the same initial conditions\n",
    "    print(f\"Initializing model, optimizer, and scheduler for Fold {fold+1}...\")\n",
    "    model = MultimodalEnsemble(num_classes) # Instantiate your model\n",
    "    model.to(device)\n",
    "\n",
    "    # Re-initialize optimizer for the new model parameters\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate) # Instantiate your optimizer\n",
    "\n",
    "    # Re-initialize scheduler\n",
    "    # Assuming T_max=25 is intended to be the total number of scheduler steps for this fold\n",
    "    # If T_max should cover the entire training process for this fold,\n",
    "    # it might need to be `len(train_loader) * num_epochs`.\n",
    "    # Based on your original code stepping per batch, we keep T_max relatively small.\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=25, verbose=False) # Set verbose=False to avoid printing every step\n",
    "\n",
    "    # Track best performance *within this fold*\n",
    "    best_f1_this_fold = -1.0\n",
    "    best_model_state_this_fold = None # To store the state_dict of the best model in this fold\n",
    "    best_threshold_this_fold = None # To store the threshold yielding the best F1 in this fold\n",
    "\n",
    "    # --- Start Epoch Loop for the current fold ---\n",
    "    print(f\"Training for {num_epochs} epochs for Fold {fold+1} started.\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # --- Training Phase ---\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "\n",
    "        # print(f\"Fold {fold+1}, Epoch {epoch+1}/{num_epochs} - Training\")\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Fold {fold+1} Epoch {epoch+1} [Train]\", leave=False)\n",
    "\n",
    "        for batch_idx, (data1, data2, data3, targets, _) in enumerate(train_pbar):\n",
    "            data1, data2, data3, targets = data1.to(device), data2.to(device), data3.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data1, data2, data3)\n",
    "\n",
    "            # Criterion creation inside batch loop using scalar pos_weight\n",
    "            # BCEWithLogitsLoss expects targets to be float\n",
    "            criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor(positive_weigh_factor, device=device))\n",
    "            loss = criterion(outputs, targets.float())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Scheduler step *after* optimizer step (per batch based on your original code's behavior)\n",
    "            scheduler.step()\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "            train_pbar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{optimizer.param_groups[0][\"lr\"]:.6f}'}) # Update progress bar\n",
    "\n",
    "        # Print average training loss for the epoch\n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        tqdm.write(f\"Fold {fold+1}, Epoch {epoch+1} - Avg Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # --- Validation Phase (after training epoch) ---\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        # print(f\"Fold {fold+1}, Epoch {epoch+1} - Validating\")\n",
    "        val_preds_sigmoid = [] # Store sigmoid outputs\n",
    "        val_targets_list = []\n",
    "        running_val_loss = 0.0\n",
    "\n",
    "        # Use torch.no_grad() during validation to save memory and computation\n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f\"Fold {fold+1} Epoch {epoch+1} [Val]\", leave=False)\n",
    "            for data1, data2, data3, targets, _ in val_pbar:\n",
    "                data1, data2, data3, targets = data1.to(device), data2.to(device), data3.to(device), targets.to(device)\n",
    "\n",
    "                outputs = model(data1, data2, data3)\n",
    "\n",
    "                # Calculate validation loss (optional) - typically use a standard BCEWithLogitsLoss without pos_weight for validation\n",
    "                val_criterion = torch.nn.BCEWithLogitsLoss() # No pos_weight for validation loss calculation\n",
    "                running_val_loss += val_criterion(outputs, targets.float()).item()\n",
    "\n",
    "                # Collect predictions (sigmoid outputs) and targets for metric calculation\n",
    "                # Move to CPU before appending - Keep as Tensors!\n",
    "                val_preds_sigmoid.append(outputs.sigmoid().cpu())\n",
    "                val_targets_list.append(targets.cpu()) # Keep as Tensors!\n",
    "\n",
    "        # Calculate average validation loss\n",
    "        avg_val_loss = running_val_loss / len(val_loader)\n",
    "        tqdm.write(f\"Fold {fold+1}, Epoch {epoch+1} - Avg Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "\n",
    "        # --- Threshold Tuning on the entire Validation Set for this epoch ---\n",
    "        # Concatenate all batch results - these are now CPU PyTorch Tensors\n",
    "        all_val_preds_sigmoid = torch.cat(val_preds_sigmoid)\n",
    "        all_val_targets = torch.cat(val_targets_list) # Targets are 0/1 integer tensors\n",
    "\n",
    "        epoch_best_f1_val = -1.0\n",
    "        epoch_best_threshold_val = -1.0\n",
    "\n",
    "        # Prepare targets for f1_score_multilabel - Keep as PyTorch Tensors!\n",
    "        # Assuming f1_score_multilabel expects boolean or float PyTorch tensors (0/1)\n",
    "        # Ensure targets are float or boolean 0/1 as expected by your f1 function\n",
    "        # If your f1 function handles integer 0/1 tensors directly, no conversion needed.\n",
    "        # Based on the sklearn example, it converts to numpy, so int 0/1 tensor is fine.\n",
    "        formatted_val_targets_tensor = all_val_targets # Assuming your f1 expects int 0/1 tensor\n",
    "\n",
    "\n",
    "        # Use a smaller loop for tqdm here as threshold loop is fast\n",
    "        threshold_pbar = tqdm(thresholds, desc=f\"Fold {fold+1} Epoch {epoch+1} [Tuning Thresholds]\", leave=False)\n",
    "        for threshold in threshold_pbar:\n",
    "            # Apply threshold to validation predictions - Keep as PyTorch Tensors (boolean)!\n",
    "            val_predictions_thresholded_tensor = (all_val_preds_sigmoid > threshold) # This results in a boolean tensor\n",
    "\n",
    "            # Calculate F1 for the current threshold\n",
    "            # Pass the PyTorch tensors directly. Ensure your f1_score_multilabel handles boolean or converts correctly.\n",
    "            current_f1_val, _, _ = f1_score_multilabel(val_predictions_thresholded_tensor, formatted_val_targets_tensor)\n",
    "\n",
    "            # Update best threshold for the current epoch's validation\n",
    "            if current_f1_val > epoch_best_f1_val:\n",
    "                epoch_best_f1_val = current_f1_val\n",
    "                epoch_best_threshold_val = threshold\n",
    "                # Update pbar postfix with the best F1 found so far in this epoch\n",
    "                threshold_pbar.set_postfix({'best_f1_epoch': f'{epoch_best_f1_val:.5f}', 'threshold': f'{epoch_best_threshold_val:.3f}'})\n",
    "\n",
    "\n",
    "        tqdm.write(f\"Fold {fold+1}, Epoch {epoch+1}. Validation best F1 for epoch: {epoch_best_f1_val:.5f} with threshold: {epoch_best_threshold_val:.3f}\")\n",
    "\n",
    "        # --- Update Best Model found *within this fold* so far ---\n",
    "        if epoch_best_f1_val > best_f1_this_fold:\n",
    "            best_f1_this_fold = epoch_best_f1_val\n",
    "            # Save model state (weights) for this fold's best epoch\n",
    "            best_model_state_this_fold = model.state_dict() # Capture the state at this best epoch\n",
    "            best_threshold_this_fold = epoch_best_threshold_val\n",
    "            tqdm.write(f\"Fold {fold+1}: New best validation F1 for this fold: {best_f1_this_fold:.5f} at Epoch {epoch+1}\")\n",
    "\n",
    "        # Optional: Add early stopping logic here based on epoch_best_f1_val for this fold\n",
    "\n",
    "\n",
    "    # --- End Epoch Loop for the current fold ---\n",
    "    # Save the best model found *within this fold* after all epochs for this fold are done\n",
    "    if best_model_state_this_fold is not None:\n",
    "        # Construct a descriptive filename including fold number, best F1, and threshold\n",
    "        save_filename = f\"fold_{fold+1}_epoch_best_f1_{best_f1_this_fold:.4f}_thresh_{best_threshold_this_fold:.3f}.pth\"\n",
    "        save_path = os.path.join(models_dir, save_filename)\n",
    "        torch.save(best_model_state_this_fold, save_path)\n",
    "        print(f\"Saved best model for Fold {fold+1} (F1: {best_f1_this_fold:.5f}, Threshold: {best_threshold_this_fold:.3f}) to {save_path}\")\n",
    "        saved_model_paths.append(save_path) # Store the path for later use\n",
    "        saved_thresholds.append(best_threshold_this_fold) # Store the threshold too\n",
    "    else:\n",
    "        print(f\"Fold {fold+1}: No model saved for this fold. Validation F1 might not have improved from initial -1.0.\")\n",
    "\n",
    "    # Optional: Clean up CUDA memory after each fold if memory is an issue\n",
    "    del model, optimizer, scheduler, train_loader, val_loader\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# --- End K-Fold Loop ---\n",
    "\n",
    "print(\"\\n--- Cross-validation finished ---\")\n",
    "print(f\"Training complete for {n_splits} folds.\")\n",
    "print(\"Saved models for ensembling (best per fold):\")\n",
    "for path in saved_model_paths:\n",
    "    print(path)\n",
    "\n",
    "# At this point, you have a list of file paths (`saved_model_paths`)\n",
    "# and corresponding best thresholds (`saved_thresholds`) for each fold's best model.\n",
    "\n",
    "# --- How to use these models for Ensembled Prediction ---\n",
    "print(\"\\n--- Example: How to load saved models for ensembling ---\")\n",
    "# To predict on new data (e.g., a test_loader), you would:\n",
    "# 1. Load each model from the saved_model_paths.\n",
    "# 2. Put each model in evaluation mode (`model.eval()`).\n",
    "# 3. Predict on the test data using each model. Collect the sigmoid outputs.\n",
    "# 4. Average the sigmoid outputs across all models for each sample/class.\n",
    "# 5. Apply a final threshold (e.g., the average of saved_thresholds, or retune on a separate set)\n",
    "#    to the averaged sigmoid outputs to get final binary predictions.\n",
    "\n",
    "# Example pseudo-code for ensembled prediction:\n",
    "# def predict_ensembled(model_paths, data_loader, num_classes, device, ensemble_threshold):\n",
    "#     all_model_preds = []\n",
    "#     for path in model_paths:\n",
    "#         model = MultimodalEnsemble(num_classes)\n",
    "#         model.load_state_dict(torch.load(path, map_location=device))\n",
    "#         model.to(device)\n",
    "#         model.eval()\n",
    "#         fold_preds = []\n",
    "#         with torch.no_grad():\n",
    "#             for data1, data2, data3, _, _ in tqdm(data_loader, desc=f\"Predicting with {os.path.basename(path)}\"):\n",
    "#                 data1, data2, data3 = data1.to(device), data2.to(device), data3.to(device)\n",
    "#                 outputs = model(data1, data2, data3)\n",
    "#                 fold_preds.append(outputs.sigmoid().cpu())\n",
    "#         all_model_preds.append(torch.cat(fold_preds))\n",
    "#         del model\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#     # Average predictions across all models\n",
    "#     ensembled_preds_sigmoid = torch.stack(all_model_preds).mean(dim=0) # Shape [num_samples, num_classes]\n",
    "\n",
    "#     # Apply final threshold\n",
    "#     final_predictions = (ensembled_preds_sigmoid > ensemble_threshold).int() # or .float() or .bool()\n",
    "\n",
    "#     return final_predictions, ensembled_preds_sigmoid\n",
    "\n",
    "# # Example usage (requires a test_loader and a final threshold)\n",
    "# # test_dataset = YourDataset(...) # Load your test dataset\n",
    "# # test_loader = DataLoader(test_dataset, batch_size=...)\n",
    "# # average_best_threshold = sum(saved_thresholds) / len(saved_thresholds) if saved_thresholds else 0.5\n",
    "# # print(f\"\\nUsing average best threshold from folds: {average_best_threshold:.3f}\")\n",
    "# # ensembled_binary_predictions, ensembled_sigmoid_outputs = predict_ensembled(\n",
    "# #     saved_model_paths, test_loader, num_classes, device, ensemble_threshold=average_best_threshold\n",
    "# # )\n",
    "# # print(\"Ensembled prediction complete.\")\n",
    "# # print(\"Example ensembled binary predictions shape:\", ensembled_binary_predictions.shape)\n",
    "# # print(\"Example ensembled sigmoid outputs shape:\", ensembled_sigmoid_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Loop\n",
    "\n",
    "Again, nothing special, just a standard inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2025-04-21T03:17:57.982460Z",
     "iopub.status.idle": "2025-04-21T03:17:57.982778Z",
     "shell.execute_reply": "2025-04-21T03:17:57.982636Z",
     "shell.execute_reply.started": "2025-04-21T03:17:57.982623Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model = MultimodalEnsemble(num_classes).to(device)\n",
    "# model.load_state_dict(torch.load(\"/home/le-chi-anh/Downloads/test geo 2024geo/best_multimodal_model_epoch_moe_routing-top-2_12_expert30_f1_0.5462.pth\",map_location=device))\n",
    "# model.eval();\n",
    "\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     all_predictions = []\n",
    "#     surveys = []\n",
    "#     top_k_indices = None\n",
    "#     for batch_idx, (data1, data2, data3, surveyID) in enumerate(test_loader):\n",
    "\n",
    "#         data1 = data1.to(device)\n",
    "#         data2 = data2.to(device)\n",
    "#         data3 = data3.to(device)\n",
    "#         targets = targets.to(device)\n",
    "\n",
    "#         outputs = model(data1, data2, data3)\n",
    "#         predictions = torch.sigmoid(outputs).cpu().numpy()\n",
    "\n",
    "#         # Sellect top-25 values as predictions\n",
    "#         top_25 = np.argsort(-predictions, axis=1)[:, :25] \n",
    "#         if top_k_indices is None:\n",
    "#             top_k_indices = top_25\n",
    "#         else:\n",
    "#             top_k_indices = np.concatenate((top_k_indices, top_25), axis=0)\n",
    "\n",
    "#         surveys.extend(surveyID.cpu().numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save prediction file! 🎉🥳🙌🤗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T03:17:57.984540Z",
     "iopub.status.idle": "2025-04-21T03:17:57.984911Z",
     "shell.execute_reply": "2025-04-21T03:17:57.984724Z",
     "shell.execute_reply.started": "2025-04-21T03:17:57.984711Z"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# data_concatenated = [' '.join(map(str, row)) for row in top_k_indices]\n",
    "\n",
    "# pd.DataFrame(\n",
    "#     {'surveyId': surveys,\n",
    "#      'predictions': data_concatenated,\n",
    "#     }).to_csv(\"submission_previous_moe_25.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     all_predictions_test = []\n",
    "#     all_surveyID_test = []\n",
    "#     all_outputs_test = []\n",
    "\n",
    "#     for batch_idx, (data0, data1, data2, surveyID) in enumerate(tqdm(test_loader)):\n",
    "#         data0 = data0.to(device)\n",
    "#         data1 = data1.to(device)\n",
    "#         data2 = data2.to(device)\n",
    "\n",
    "#         outputs = model(data0, data1, data2)\n",
    "#         outputs_np = torch.sigmoid(outputs).cpu().numpy()\n",
    "#         preds = (outputs_np > 0.2).astype(int)\n",
    "\n",
    "#         all_predictions_test.extend(preds)\n",
    "#         all_surveyID_test.extend(surveyID.numpy())\n",
    "#         all_outputs_test.append(outputs)\n",
    "\n",
    "#     all_predictions_test = np.array(all_predictions_test)\n",
    "#     all_surveyID_test = np.array(all_surveyID_test)\n",
    "#     all_outputs_test = torch.cat(all_outputs_test, dim=0)\n",
    "\n",
    "#     # Mapping labels\n",
    "\n",
    "#     data_concatenated = []\n",
    "#     for row in all_predictions_test:\n",
    "#         labels = np.where(row == 1)[0]\n",
    "#         labels_str = ' '.join(map(str, labels))\n",
    "#         data_concatenated.append(labels_str)\n",
    "\n",
    "#     submission_df = pd.DataFrame({\n",
    "#         'surveyId': all_surveyID_test,\n",
    "#         'predictions': data_concatenated\n",
    "#     })\n",
    "\n",
    "#     submission_df.to_csv(\"submission_test_none_thresh_cross_validation_moe_routing_top-2_12_expert.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:08<00:00,  4.57it/s]\n",
      "100%|██████████| 37/37 [00:08<00:00,  4.57it/s]\n",
      "100%|██████████| 37/37 [00:08<00:00,  4.62it/s]\n",
      "100%|██████████| 37/37 [00:08<00:00,  4.60it/s]\n",
      "100%|██████████| 37/37 [00:08<00:00,  4.58it/s]\n",
      "100%|██████████| 5/5 [00:44<00:00,  8.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done. Submission saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "all_predictions = []\n",
    "all_survey_ids = []\n",
    "\n",
    "for i_fold in tqdm(range(5)):\n",
    "    # Load model cho fold này\n",
    "    model = MultimodalEnsemble(num_classes=num_classes).to(device)\n",
    "    model.load_state_dict(torch.load(f\"/home/le-chi-anh/Downloads/test geo 2024geo/saved_fold_models_2/fold_{i_fold+1}_epoch_best_f1_thresh_0.200.pth\"))\n",
    "    model.eval()\n",
    "\n",
    "    fold_preds = []\n",
    "    fold_survey_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (value_features, landsat, bioclim, survey_id) in enumerate(tqdm(test_loader)):\n",
    "            value_features = value_features.to(device)\n",
    "            landsat = landsat.to(device)\n",
    "            bioclim = bioclim.to(device)\n",
    "\n",
    "            outputs = model(value_features, landsat, bioclim)\n",
    "            outputs_np = torch.sigmoid(outputs).cpu().numpy()\n",
    "\n",
    "            fold_preds.append(outputs_np)\n",
    "            fold_survey_ids.extend(survey_id.numpy())\n",
    "\n",
    "    fold_preds = np.concatenate(fold_preds, axis=0)\n",
    "    all_predictions.append(fold_preds)\n",
    "    all_survey_ids.append(fold_survey_ids)\n",
    "\n",
    "# Stack prediction của tất cả folds (shape: [5, N, num_classes])\n",
    "all_predictions = np.stack(all_predictions, axis=0)\n",
    "\n",
    "# Average predict over folds\n",
    "avg_predictions = np.mean(all_predictions, axis=0)  # shape: [N, num_classes]\n",
    "\n",
    "# Apply threshold\n",
    "threshold = 0.2\n",
    "binary_preds = (avg_predictions > threshold).astype(int)\n",
    "\n",
    "# survey IDs (chỉ cần lấy 1 lần, vì tất cả fold survey_id đều giống nhau)\n",
    "survey_ids = all_survey_ids[0]\n",
    "\n",
    "# Mapping labels thành string\n",
    "data_concatenated = []\n",
    "for row in binary_preds:\n",
    "    labels = np.where(row == 1)[0]\n",
    "    labels_str = ' '.join(map(str, labels))\n",
    "    data_concatenated.append(labels_str)\n",
    "\n",
    "# Save submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'surveyId': survey_ids,\n",
    "    'predictions': data_concatenated\n",
    "})\n",
    "\n",
    "submission_df.to_csv(\"submission_test_none_thresh_cross_validation_8_expert_2.csv\", index=False)\n",
    "\n",
    "print(\"✅ Done. Submission saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8171035,
     "sourceId": 64733,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
